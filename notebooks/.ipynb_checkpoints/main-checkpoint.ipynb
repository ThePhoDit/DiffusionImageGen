{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI System\n",
    "\n",
    "This notebook acts as an index for all other available notebooks.\n",
    "\n",
    "> Please, before proceeding make sure all packages from `requirements.txt` are installed.\n",
    "\n",
    "- [Configurable Generation & Generation Examples on Pretrained models \\[MNIST (3 channels) and CIFAR-10\\]](./train_and_generate.ipynb)\n",
    "- [Imputation](./imputation.ipynb)\n",
    "- [Measures of Quality](./quality.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the module work?\n",
    "\n",
    "The module is primarily thought to work as a command line utility through the `main.py` script. It offers a wide range of parameters to control what you want to do and the parameteres for each action: `train`, `generate`, `impute` or  `evaluate`. Most arguments have some good default values.\n",
    "\n",
    "On the notebooks above you will find already made commands to test all available features in this Python module.\n",
    "\n",
    "You can check all the options as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "usage: main.py [-h] [--dataset {mnist,cifar10}]\n",
      "               [--mode {train,generate,impute,evaluate}]\n",
      "               [--process {ve,vp,subvp}] [--schedule {linear,cosine}]\n",
      "               [--beta_min BETA_MIN] [--beta_max BETA_MAX]\n",
      "               [--cosine_s COSINE_S] [--target_class TARGET_CLASS]\n",
      "               [--use_class_condition] [--filter_dataset] [--epochs EPOCHS]\n",
      "               [--batch_size BATCH_SIZE] [--lr LR] [--model_path MODEL_PATH]\n",
      "               [--num_generate NUM_GENERATE] [--gen_steps GEN_STEPS]\n",
      "               [--image_channels IMAGE_CHANNELS] [--image_size IMAGE_SIZE]\n",
      "               [--gen_eps GEN_EPS] [--sampler {euler,pc,ode,ei}]\n",
      "               [--pc_snr PC_SNR] [--pc_corrector_steps PC_CORRECTOR_STEPS]\n",
      "               [--ode_early_stop_time ODE_EARLY_STOP_TIME] [--no_ode_rk4]\n",
      "               [--input_image INPUT_IMAGE] [--mask_image MASK_IMAGE]\n",
      "               [--output_image OUTPUT_IMAGE] [--impute_steps IMPUTE_STEPS]\n",
      "               [--jump_length JUMP_LENGTH] [--jump_n_sample JUMP_N_SAMPLE]\n",
      "               [--num_eval_samples NUM_EVAL_SAMPLES]\n",
      "               [--eval_batch_size EVAL_BATCH_SIZE] [--notebook_tqdm]\n",
      "\n",
      "Train or generate images (MNIST/CIFAR-10) with selectable diffusion process.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --dataset {mnist,cifar10}\n",
      "                        Dataset to use: 'mnist' or 'cifar10'.\n",
      "  --mode {train,generate,impute,evaluate}\n",
      "                        Operation mode.\n",
      "  --process {ve,vp,subvp}\n",
      "                        Diffusion process type.\n",
      "  --schedule {linear,cosine}\n",
      "                        VP/Sub-VP Schedule.\n",
      "  --beta_min BETA_MIN   Linear schedule beta_min.\n",
      "  --beta_max BETA_MAX   Linear schedule beta_max.\n",
      "  --cosine_s COSINE_S   Cosine schedule s.\n",
      "  --target_class TARGET_CLASS\n",
      "                        Target class index (0-9).\n",
      "  --use_class_condition\n",
      "                        Enable class conditioning.\n",
      "  --filter_dataset      Filter training dataset to target_class.\n",
      "  --epochs EPOCHS       Training epochs (default 50).\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size.\n",
      "  --lr LR               Learning rate (default 1e-4).\n",
      "  --model_path MODEL_PATH\n",
      "                        Path to load/save model. Default determined by\n",
      "                        dataset/config.\n",
      "  --num_generate NUM_GENERATE\n",
      "                        Number of images to generate.\n",
      "  --gen_steps GEN_STEPS\n",
      "                        Sampler steps for generation.\n",
      "  --image_channels IMAGE_CHANNELS\n",
      "                        Image channels (default: 1 for MNIST, 3 for CIFAR).\n",
      "  --image_size IMAGE_SIZE\n",
      "                        Image size (default: 28 for MNIST, 32 for CIFAR).\n",
      "  --gen_eps GEN_EPS     Reverse SDE integration endpoint.\n",
      "  --sampler {euler,pc,ode,ei}\n",
      "                        Sampler type.\n",
      "  --pc_snr PC_SNR       PC sampler SNR.\n",
      "  --pc_corrector_steps PC_CORRECTOR_STEPS\n",
      "                        PC sampler corrector steps.\n",
      "  --ode_early_stop_time ODE_EARLY_STOP_TIME\n",
      "                        ODE sampler early stop time.\n",
      "  --no_ode_rk4          Use Euler instead of RK4 for ODE sampler.\n",
      "  --input_image INPUT_IMAGE\n",
      "                        Input image for imputation.\n",
      "  --mask_image MASK_IMAGE\n",
      "                        Mask image for imputation.\n",
      "  --output_image OUTPUT_IMAGE\n",
      "                        Output path for imputed image.\n",
      "  --impute_steps IMPUTE_STEPS\n",
      "                        Imputation sampler steps.\n",
      "  --jump_length JUMP_LENGTH\n",
      "                        Repaint jump length N.\n",
      "  --jump_n_sample JUMP_N_SAMPLE\n",
      "                        Repaint jump sample size R.\n",
      "  --num_eval_samples NUM_EVAL_SAMPLES\n",
      "                        Number of samples for evaluation.\n",
      "  --eval_batch_size EVAL_BATCH_SIZE\n",
      "                        Batch size for evaluation.\n",
      "  --notebook_tqdm       Use tqdm.notebook.\n"
     ]
    }
   ],
   "source": [
    "!cd .. && python main.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find some trained models for MNIST (using 3 channels) and CIFAR-10 datasets. These are the ones we will be using for the showcases in the other notebooks.\n",
    "\n",
    "**For MNIST (3 channels)**\n",
    "- `ve_50_conditional`: Variance Exploding model trained on 50 epochs.\n",
    "- `ve_uncond_epoch50`: Unconditional model trained with Variance Exploding trained on 50 epochs.\n",
    "- `vp_linear_40_conditional`: Variance Preserving model with a linear noise schedule trained on 40 epochs.\n",
    "- `vp_cosine_50_epoch50`: Variance Preserving model with a cosine noise schedule trained on 50 epochs.\n",
    "- `subvp_linear`: Sub-Variance Preserving model trained on 20 epochs.\n",
    "\n",
    "**For CIFAR-10**\n",
    "- `ve_100_conditional`: Variance Exploding model trained on 100 epochs.\n",
    "- `vp_cosine_100_conditional`: Variance Preserving model with a cosine noise schedule trained on 100 epochs.\n",
    "- `subvp_cosine_100_conditional`: Sub-Variance Preserving model with a cosine noise schedule trained on 100 epochs.\n",
    "- `ve_unconditional_class_1_epoch150`: Variance Explosing model trained only on cars. Unconditional model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
