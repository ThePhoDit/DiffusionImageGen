<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>diffusion.samplers.euler_maruyama API documentation</title>
<meta name="description" content="Simulate Gaussian processes …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>diffusion.samplers.euler_maruyama</code></h1>
</header>
<section id="section-intro">
<p>Simulate Gaussian processes.</p>
<p>@author: <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#108;&#98;&#101;&#114;&#116;&#111;&#46;&#115;&#117;&#97;&#114;&#101;&#122;&#64;&#117;&#97;&#109;&#46;&#101;&#115;">&#97;&#108;&#98;&#101;&#114;&#116;&#111;&#46;&#115;&#117;&#97;&#114;&#101;&#122;&#64;&#117;&#97;&#109;&#46;&#101;&#115;</a></p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="diffusion.samplers.euler_maruyama.euler_maruyama_integrator"><code class="name flex">
<span>def <span class="ident">euler_maruyama_integrator</span></span>(<span>x_0: Tensor,<br>t_0: float,<br>t_end: float,<br>n_steps: int,<br>drift_coefficient: Callable[float, float],<br>diffusion_coefficient: Callable[float],<br>seed: Union[int, None] = None) ‑> Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def euler_maruyama_integrator(
    x_0: Tensor,
    t_0: float,
    t_end: float,
    n_steps: int,
    drift_coefficient: Callable[float, float],
    diffusion_coefficient: Callable[float],
    seed: Union[int, None] = None,
) -&gt; Tensor:
    &#34;&#34;&#34;Euler-Maruyama integrator (approximate)

        Args:
        x_0: The initial images of dimensions
            (batch_size, n_channels, image_height, image_width)
        t_0: float,
        t_end: endpoint of the integration interval
        n_steps: number of integration steps
        drift_coefficient: Function of :math`(x(t), t)` that defines the drift term
        diffusion_coefficient: Function of :math`(t)` that defines the diffusion term
        seed: Seed for the random number generator

    Returns:
        x_t: Trajectories that result from the integration of the SDE.
                The shape is (*np.shape(x_0), (n_steps + 1))

    Notes:
        The implementation is fully vectorized except for a loop over time.

    Examples:
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; drift_coefficient = lambda x_t, t: - x_t
        &gt;&gt;&gt; diffusion_coefficient = lambda t: torch.ones_like(t)
        &gt;&gt;&gt; x_0 = torch.tensor(np.reshape(np.arange(120), (2, 3, 5, 4)))
        &gt;&gt;&gt; t_0, t_end = 0.0, 3.0
        &gt;&gt;&gt; n_steps = 6
        &gt;&gt;&gt; times, x_t = euler_maruyama_integrator(
        ...     x_0, t_0, t_end, n_steps, drift_coefficient, diffusion_coefficient,
        ... )
        &gt;&gt;&gt; print(times)
        tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000])
        &gt;&gt;&gt; print(np.shape(x_t))
        torch.Size([2, 3, 5, 4, 7])
    &#34;&#34;&#34;
    device = x_0.device

    # Create a tensor of time points from t_0 to t_end
    times = torch.linspace(t_0, t_end, n_steps + 1, device=device)
    dt = times[1] - times[0]

    # Initialize the tensor to store the trajectories
    x_t = torch.tensor(
        np.empty((*np.shape(x_0), len(times))),
        dtype=torch.float32,
        device=device,
    )
    x_t[..., 0] = x_0  # Set the initial condition

    z = torch.randn_like(x_t)  # Generate random noise for the diffusion term

    # Loop over each time step (except the last one)
    for n, t in enumerate(times[:-1]):
        t = (
            torch.ones(x_0.shape[0], device=device) * t
        )  # Create a tensor of the current time
        # Update the state using the Euler-Maruyama method
        x_t[..., n + 1] = (
            x_t[..., n]  # Current state
            + drift_coefficient(x_t[..., n], t) * dt  # Drift term
            + diffusion_coefficient(t).view(
                -1, 1, 1, 1
            )  # Diffusion term reshaped for broadcasting
            * torch.sqrt(
                torch.abs(dt)
            )  # Scale by the square root of the time step
            * z[..., n]  # Add random noise
        )

    return times, x_t</code></pre>
</details>
<div class="desc"><p>Euler-Maruyama integrator (approximate)</p>
<pre><code>Args:
x_0: The initial images of dimensions
    (batch_size, n_channels, image_height, image_width)
t_0: float,
t_end: endpoint of the integration interval
n_steps: number of integration steps
drift_coefficient: Function of :math&lt;code&gt;(x(t), t)&lt;/code&gt; that defines the drift term
diffusion_coefficient: Function of :math&lt;code&gt;(t)&lt;/code&gt; that defines the diffusion term
seed: Seed for the random number generator
</code></pre>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x_t</code></dt>
<dd>Trajectories that result from the integration of the SDE.
The shape is (*np.shape(x_0), (n_steps + 1))</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The implementation is fully vectorized except for a loop over time.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; drift_coefficient = lambda x_t, t: - x_t
&gt;&gt;&gt; diffusion_coefficient = lambda t: torch.ones_like(t)
&gt;&gt;&gt; x_0 = torch.tensor(np.reshape(np.arange(120), (2, 3, 5, 4)))
&gt;&gt;&gt; t_0, t_end = 0.0, 3.0
&gt;&gt;&gt; n_steps = 6
&gt;&gt;&gt; times, x_t = euler_maruyama_integrator(
...     x_0, t_0, t_end, n_steps, drift_coefficient, diffusion_coefficient,
... )
&gt;&gt;&gt; print(times)
tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000])
&gt;&gt;&gt; print(np.shape(x_t))
torch.Size([2, 3, 5, 4, 7])
</code></pre></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="diffusion.samplers.euler_maruyama.DiffussionProcess"><code class="flex name class">
<span>class <span class="ident">DiffussionProcess</span></span>
<span>(</span><span>drift_coefficient: Callable[float, float] = &lt;function DiffussionProcess.&lt;lambda&gt;&gt;,<br>diffusion_coefficient: Callable[float] = &lt;function DiffussionProcess.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DiffussionProcess:
    &#34;&#34;&#34;Base class for defining diffusion process coefficients.&#34;&#34;&#34;

    def __init__(
        self,
        drift_coefficient: Callable[float, float] = lambda x_t, t: 0.0,
        diffusion_coefficient: Callable[float] = lambda t: 1.0,
    ):
        self.drift_coefficient = drift_coefficient
        self.diffusion_coefficient = diffusion_coefficient

    def loss_function(self, score_model, x_0, eps=1e-5):
        &#34;&#34;&#34;
        Computes the loss for the diffusion process using a time-dependent score model.

        This function calculates the loss based on the difference between the predicted scores
        from the score model and the true scores derived from the original data. The loss is
        computed as the mean squared error of the sum of the scaled predicted score and Gaussian
        noise, which quantifies how well the score model approximates the gradients of the log
        probability density function of the data.

        Args:
            score_model (Callable): A time-dependent score model that takes in the current state
                                    `x_t` and time `t`, returning the estimated score (gradient
                                    of the log probability density).
            x_0 (Tensor): The original data batch, expected shape (batch_size, C, H, W) or similar.
            eps (float, optional): A small constant to avoid division by zero or log of zero
                                   when sampling times. Default is 1e-5.

        Returns:
            Tensor: The computed loss value, which can be used for backpropagation to update
                    the parameters of the score model.

        Example:
            &gt;&gt;&gt; score_model = ...  # Assume a pre-defined score model
            &gt;&gt;&gt; x_0 = torch.randn(32, 3, 64, 64)  # Example batch of images
            &gt;&gt;&gt; loss = self.loss_function(score_model, x_0)
            &gt;&gt;&gt; print(loss.item())  # Output the loss value

        Notes:
            The loss function is crucial for training the score model, as it guides the optimization
            process to improve the model&#39;s ability to estimate the score accurately. The choice of
            loss function may vary depending on the specific application and desired properties of
            the diffusion process. The implementation assumes a simple forward diffusion process
            where noise is added to the original data based on a time-dependent scaling factor.
        &#34;&#34;&#34;

        # 1. Sample times t in [eps, 1]
        t = torch.rand(x_0.shape[0], device=x_0.device) * (1.0 - eps) + eps

        # 2. Sample Gaussian noise
        noise = torch.randn_like(x_0)

        # 3. Compute sigma(t). This might be a direct function or a learned schedule.
        #    Suppose sigma(t) returns shape (batch_size,).
        sigma = self.diffusion_coefficient(t)  # e.g. shape (batch_size,)

        # 4. Reshape sigma to broadcast with x_0
        #    e.g. if x_0 is (batch_size, C, H, W), make sigma (batch_size, 1, 1, 1).
        sigma = sigma.view(x_0.shape[0], *([1] * (x_0.dim() - 1)))

        # 5. Create the noisy sample x_t
        #    If your forward diffusion is x_t = x_0 + sigma(t)*noise (simple case):
        x_t = x_0 + sigma * noise

        # 6. Get the model&#39;s predicted score s(x_t, t)
        score = score_model(x_t, t)

        # 7. According to class notes, the loss is:
        #       || sigma(t) * score + noise ||^2
        #    so we just implement that directly:
        #    We sum over all non-batch dimensions, then average over the batch.
        mse_per_example = torch.sum(
            (sigma * score + noise) ** 2, dim=list(range(1, x_0.dim()))
        )
        loss = torch.mean(mse_per_example)

        return loss</code></pre>
</details>
<div class="desc"><p>Base class for defining diffusion process coefficients.</p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="diffusion.samplers.euler_maruyama.GaussianDiffussionProcess" href="#diffusion.samplers.euler_maruyama.GaussianDiffussionProcess">GaussianDiffussionProcess</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="diffusion.samplers.euler_maruyama.DiffussionProcess.loss_function"><code class="name flex">
<span>def <span class="ident">loss_function</span></span>(<span>self, score_model, x_0, eps=1e-05)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loss_function(self, score_model, x_0, eps=1e-5):
    &#34;&#34;&#34;
    Computes the loss for the diffusion process using a time-dependent score model.

    This function calculates the loss based on the difference between the predicted scores
    from the score model and the true scores derived from the original data. The loss is
    computed as the mean squared error of the sum of the scaled predicted score and Gaussian
    noise, which quantifies how well the score model approximates the gradients of the log
    probability density function of the data.

    Args:
        score_model (Callable): A time-dependent score model that takes in the current state
                                `x_t` and time `t`, returning the estimated score (gradient
                                of the log probability density).
        x_0 (Tensor): The original data batch, expected shape (batch_size, C, H, W) or similar.
        eps (float, optional): A small constant to avoid division by zero or log of zero
                               when sampling times. Default is 1e-5.

    Returns:
        Tensor: The computed loss value, which can be used for backpropagation to update
                the parameters of the score model.

    Example:
        &gt;&gt;&gt; score_model = ...  # Assume a pre-defined score model
        &gt;&gt;&gt; x_0 = torch.randn(32, 3, 64, 64)  # Example batch of images
        &gt;&gt;&gt; loss = self.loss_function(score_model, x_0)
        &gt;&gt;&gt; print(loss.item())  # Output the loss value

    Notes:
        The loss function is crucial for training the score model, as it guides the optimization
        process to improve the model&#39;s ability to estimate the score accurately. The choice of
        loss function may vary depending on the specific application and desired properties of
        the diffusion process. The implementation assumes a simple forward diffusion process
        where noise is added to the original data based on a time-dependent scaling factor.
    &#34;&#34;&#34;

    # 1. Sample times t in [eps, 1]
    t = torch.rand(x_0.shape[0], device=x_0.device) * (1.0 - eps) + eps

    # 2. Sample Gaussian noise
    noise = torch.randn_like(x_0)

    # 3. Compute sigma(t). This might be a direct function or a learned schedule.
    #    Suppose sigma(t) returns shape (batch_size,).
    sigma = self.diffusion_coefficient(t)  # e.g. shape (batch_size,)

    # 4. Reshape sigma to broadcast with x_0
    #    e.g. if x_0 is (batch_size, C, H, W), make sigma (batch_size, 1, 1, 1).
    sigma = sigma.view(x_0.shape[0], *([1] * (x_0.dim() - 1)))

    # 5. Create the noisy sample x_t
    #    If your forward diffusion is x_t = x_0 + sigma(t)*noise (simple case):
    x_t = x_0 + sigma * noise

    # 6. Get the model&#39;s predicted score s(x_t, t)
    score = score_model(x_t, t)

    # 7. According to class notes, the loss is:
    #       || sigma(t) * score + noise ||^2
    #    so we just implement that directly:
    #    We sum over all non-batch dimensions, then average over the batch.
    mse_per_example = torch.sum(
        (sigma * score + noise) ** 2, dim=list(range(1, x_0.dim()))
    )
    loss = torch.mean(mse_per_example)

    return loss</code></pre>
</details>
<div class="desc"><p>Computes the loss for the diffusion process using a time-dependent score model.</p>
<p>This function calculates the loss based on the difference between the predicted scores
from the score model and the true scores derived from the original data. The loss is
computed as the mean squared error of the sum of the scaled predicted score and Gaussian
noise, which quantifies how well the score model approximates the gradients of the log
probability density function of the data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>score_model</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A time-dependent score model that takes in the current state
<code>x_t</code> and time <code>t</code>, returning the estimated score (gradient
of the log probability density).</dd>
<dt><strong><code>x_0</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The original data batch, expected shape (batch_size, C, H, W) or similar.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>A small constant to avoid division by zero or log of zero
when sampling times. Default is 1e-5.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>The computed loss value, which can be used for backpropagation to update
the parameters of the score model.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; score_model = ...  # Assume a pre-defined score model
&gt;&gt;&gt; x_0 = torch.randn(32, 3, 64, 64)  # Example batch of images
&gt;&gt;&gt; loss = self.loss_function(score_model, x_0)
&gt;&gt;&gt; print(loss.item())  # Output the loss value
</code></pre>
<h2 id="notes">Notes</h2>
<p>The loss function is crucial for training the score model, as it guides the optimization
process to improve the model's ability to estimate the score accurately. The choice of
loss function may vary depending on the specific application and desired properties of
the diffusion process. The implementation assumes a simple forward diffusion process
where noise is added to the original data based on a time-dependent scaling factor.</p></div>
</dd>
</dl>
</dd>
<dt id="diffusion.samplers.euler_maruyama.GaussianDiffussionProcess"><code class="flex name class">
<span>class <span class="ident">GaussianDiffussionProcess</span></span>
<span>(</span><span>drift_coefficient: Callable[float, float] = &lt;function GaussianDiffussionProcess.&lt;lambda&gt;&gt;,<br>diffusion_coefficient: Callable[float] = &lt;function GaussianDiffussionProcess.&lt;lambda&gt;&gt;,<br>mu_t: Callable[float, float] = &lt;function GaussianDiffussionProcess.&lt;lambda&gt;&gt;,<br>sigma_t: Callable[float] = &lt;function GaussianDiffussionProcess.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GaussianDiffussionProcess(DiffussionProcess):
    &#34;&#34;&#34;
    A class representing a Gaussian diffusion process, which models the evolution of a stochastic process
    with a constant drift and diffusion coefficient. This process can be used to simulate Brownian motion
    or other diffusion phenomena in a variety of applications.

    Attributes:
        drift_coefficient (Callable): A function that defines the drift term of the process.
        diffusion_coefficient (Callable): A function that defines the diffusion term of the process.
        mu_t (Callable): A function that computes the expected value of the process at time t given the initial state x_0.
        sigma_t (Callable): A function that computes the standard deviation of the process at time t.

    Args:
        drift_coefficient (Callable): A function of the form `f(x_t, t)` that returns the drift term at state `x_t` and time `t`.
        diffusion_coefficient (Callable): A function of the form `g(t)` that returns the diffusion coefficient at time `t`.
        mu_t (Callable): A function of the form `h(x_0, t)` that computes the expected value of the process at time `t`.
        sigma_t (Callable): A function of the form `k(t)` that computes the standard deviation of the process at time `t`.

    Example 1:
        &gt;&gt;&gt; mu, sigma = 1.5, 2.0
        &gt;&gt;&gt; bm = GaussianDiffussionProcess(
        ...     drift_coefficient=lambda x_t, t: mu,
        ...     diffusion_coefficient=lambda t: sigma,
        ...     mu_t=lambda x_0, t: x_0 + mu*t,
        ...     sigma_t=lambda t: np.sqrt(2.0 * t),
        ... )
        &gt;&gt;&gt; print(bm.drift_coefficient(x_t=3.0, t=10.0))  # Output: 1.5
        &gt;&gt;&gt; print(bm.diffusion_coefficient(t=10.0))  # Output: 2.0
        &gt;&gt;&gt; print(bm.mu_t(x_0=3.0, t=10.0), bm.sigma_t(t=10.0))  # Output: 18.0 4.47213595499958

    Notes:
        This class inherits from the `DiffussionProcess` base class and implements the necessary methods
        to define a Gaussian diffusion process. The drift and diffusion coefficients can be constant or
        time-dependent functions, allowing for flexibility in modeling various stochastic processes.
    &#34;&#34;&#34;

    kind = &#34;Gaussian&#34;

    def __init__(
        self,
        drift_coefficient: Callable[float, float] = lambda x_t, t: 0.0,
        diffusion_coefficient: Callable[float] = lambda t: 1.0,
        mu_t: Callable[float, float] = lambda x_0, t: x_0,
        sigma_t: Callable[float] = lambda t: np.sqrt(t),
    ):
        self.drift_coefficient = drift_coefficient
        self.diffusion_coefficient = diffusion_coefficient
        self.mu_t = mu_t
        self.sigma_t = sigma_t

    def loss_function(self, score_model, x_0, eps=1e-5):
        &#34;&#34;&#34;
        Computes the loss for the diffusion process using a time-dependent score model.

        This function calculates the loss based on the difference between the predicted scores
        from the score model and the true scores derived from the original data. The loss is
        computed as the mean squared error of the sum of the scaled predicted score and Gaussian
        noise, which quantifies how well the score model approximates the gradients of the log
        probability density function of the data.

        Args:
            score_model (Callable): A time-dependent score model that takes in the current state
                                    `x_t` and time `t`, returning the estimated score (gradient
                                    of the log probability density).
            x_0 (Tensor): The original data batch, expected shape (batch_size, C, H, W) or similar.
            eps (float, optional): A small constant to avoid division by zero or log of zero
                                   when sampling times. Default is 1e-5.

        Returns:
            Tensor: The computed loss value, which can be used for backpropagation to update
                    the parameters of the score model.

        Example:
            &gt;&gt;&gt; score_model = ...  # Assume a pre-defined score model
            &gt;&gt;&gt; x_0 = torch.randn(32, 3, 64, 64)  # Example batch of images
            &gt;&gt;&gt; loss = self.loss_function(score_model, x_0)
            &gt;&gt;&gt; print(loss.item())  # Output the loss value

        Notes:
            The loss function is crucial for training the score model, as it guides the optimization
            process to improve the model&#39;s ability to estimate the score accurately. The choice of
            loss function may vary depending on the specific application and desired properties of
            the diffusion process. The implementation assumes a simple forward diffusion process
            where noise is added to the original data based on a time-dependent scaling factor.
        &#34;&#34;&#34;

        # 1. Sample times t in [eps, 1]
        t = torch.rand(x_0.shape[0], device=x_0.device) * (1.0 - eps) + eps

        # 2. Sample Gaussian noise
        noise = torch.randn_like(x_0)

        # 3. Compute sigma(t). This might be a direct function or a learned schedule.
        #    Suppose sigma(t) returns shape (batch_size,).
        sigma = self.sigma_t(t)  # e.g. shape (batch_size,)

        # 4. Reshape sigma to broadcast with x_0
        #    e.g. if x_0 is (batch_size, C, H, W), make sigma (batch_size, 1, 1, 1).
        sigma = sigma.view(x_0.shape[0], *([1] * (x_0.dim() - 1)))

        # 5. Create the noisy sample x_t
        #    If your forward diffusion is x_t = x_0 + sigma(t)*noise (simple case):
        x_t = x_0 + sigma * noise

        # 6. Get the model&#39;s predicted score s(x_t, t)
        score = score_model(x_t, t)

        # 7. According to class notes, the loss is:
        #       || sigma(t) * score + noise ||^2
        #    so we just implement that directly:
        #    We sum over all non-batch dimensions, then average over the batch.
        mse_per_example = torch.sum(
            (sigma * score + noise) ** 2, dim=list(range(1, x_0.dim()))
        )
        loss = torch.mean(mse_per_example)

        return loss</code></pre>
</details>
<div class="desc"><p>A class representing a Gaussian diffusion process, which models the evolution of a stochastic process
with a constant drift and diffusion coefficient. This process can be used to simulate Brownian motion
or other diffusion phenomena in a variety of applications.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>drift_coefficient</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function that defines the drift term of the process.</dd>
<dt><strong><code>diffusion_coefficient</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function that defines the diffusion term of the process.</dd>
<dt><strong><code>mu_t</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function that computes the expected value of the process at time t given the initial state x_0.</dd>
<dt><strong><code>sigma_t</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function that computes the standard deviation of the process at time t.</dd>
</dl>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drift_coefficient</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function of the form <code>f(x_t, t)</code> that returns the drift term at state <code>x_t</code> and time <code>t</code>.</dd>
<dt><strong><code>diffusion_coefficient</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function of the form <code>g(t)</code> that returns the diffusion coefficient at time <code>t</code>.</dd>
<dt><strong><code>mu_t</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function of the form <code>h(x_0, t)</code> that computes the expected value of the process at time <code>t</code>.</dd>
<dt><strong><code>sigma_t</code></strong> :&ensp;<code>Callable</code></dt>
<dd>A function of the form <code>k(t)</code> that computes the standard deviation of the process at time <code>t</code>.</dd>
</dl>
<p>Example 1:
&gt;&gt;&gt; mu, sigma = 1.5, 2.0
&gt;&gt;&gt; bm = GaussianDiffussionProcess(
&hellip;
drift_coefficient=lambda x_t, t: mu,
&hellip;
diffusion_coefficient=lambda t: sigma,
&hellip;
mu_t=lambda x_0, t: x_0 + mu*t,
&hellip;
sigma_t=lambda t: np.sqrt(2.0 * t),
&hellip; )
&gt;&gt;&gt; print(bm.drift_coefficient(x_t=3.0, t=10.0))
# Output: 1.5
&gt;&gt;&gt; print(bm.diffusion_coefficient(t=10.0))
# Output: 2.0
&gt;&gt;&gt; print(bm.mu_t(x_0=3.0, t=10.0), bm.sigma_t(t=10.0))
# Output: 18.0 4.47213595499958</p>
<h2 id="notes">Notes</h2>
<p>This class inherits from the <code><a title="diffusion.samplers.euler_maruyama.DiffussionProcess" href="#diffusion.samplers.euler_maruyama.DiffussionProcess">DiffussionProcess</a></code> base class and implements the necessary methods
to define a Gaussian diffusion process. The drift and diffusion coefficients can be constant or
time-dependent functions, allowing for flexibility in modeling various stochastic processes.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="diffusion.samplers.euler_maruyama.DiffussionProcess" href="#diffusion.samplers.euler_maruyama.DiffussionProcess">DiffussionProcess</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="diffusion.samplers.euler_maruyama.GaussianDiffussionProcess.kind"><code class="name">var <span class="ident">kind</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="diffusion.samplers.euler_maruyama.DiffussionProcess" href="#diffusion.samplers.euler_maruyama.DiffussionProcess">DiffussionProcess</a></b></code>:
<ul class="hlist">
<li><code><a title="diffusion.samplers.euler_maruyama.DiffussionProcess.loss_function" href="#diffusion.samplers.euler_maruyama.DiffussionProcess.loss_function">loss_function</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="diffusion.samplers" href="index.html">diffusion.samplers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="diffusion.samplers.euler_maruyama.euler_maruyama_integrator" href="#diffusion.samplers.euler_maruyama.euler_maruyama_integrator">euler_maruyama_integrator</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="diffusion.samplers.euler_maruyama.DiffussionProcess" href="#diffusion.samplers.euler_maruyama.DiffussionProcess">DiffussionProcess</a></code></h4>
<ul class="">
<li><code><a title="diffusion.samplers.euler_maruyama.DiffussionProcess.loss_function" href="#diffusion.samplers.euler_maruyama.DiffussionProcess.loss_function">loss_function</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="diffusion.samplers.euler_maruyama.GaussianDiffussionProcess" href="#diffusion.samplers.euler_maruyama.GaussianDiffussionProcess">GaussianDiffussionProcess</a></code></h4>
<ul class="">
<li><code><a title="diffusion.samplers.euler_maruyama.GaussianDiffussionProcess.kind" href="#diffusion.samplers.euler_maruyama.GaussianDiffussionProcess.kind">kind</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
